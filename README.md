# MAMMOGRAPHY-AND-ULTRASOUND-BASED-DUAL-MODALITY-CLASSIFICATION-OF-BREAST-CANCER-
MAMMOGRAPHY AND ULTRASOUND BASED DUAL MODALITY CLASSIFICATION OF BREAST CANCER USING A HYBRID DEEP LEARNING APPROACH
Breast cancer remains a major health problem worldwide and requires accurate and
timely diagnostic methods for effective treatment. Mammography and ultrasound
imaging are widely used modalities for breast cancer detection and each has its own
advantages and limitations. In this study, we propose a dual-modality classification
approach for breast cancer using a hybrid deep learning framework. The proposed
method aims to improve classification accuracy and reduce false positives by
integrating a convolutional neural network (CNN) for mammography and a recurrent
neural network (RNN) for ultrasound images. First, preprocessing techniques are
applied to standardize and improve the quality of mammography and ultrasound

images. CNN-based models are then trained on mammography images to extract high-
level features indicative of malignancy, while RNN based models process ultrasound

images to extract temporal patterns and texture information. The extracted features
from both modalities are combined at the decision level using a hybrid fusion strategy
to provide a comprehensive assessment of breast cancer probability. Experimental
results on large datasets show that the proposed hybrid deep learning approach achieves

superior classification performance compared to single modality methods. The dual-
modality classification model exhibits high sensitivity, specificity and accuracy,

providing a robust and reliable tool for breast cancer diagnosis. The integration of
mammography and ultrasound images through hybrid deep learning provides a
promising tool to improve early detection and characterization of breast cancer,
ultimately facilitating more personalized and effective patient care.
